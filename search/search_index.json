{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CovidRt.jl \u00b6 CovidRt.CovidRt CovidRt.lagdiff CovidRt.smooth # CovidRt.CovidRt \u2014 Module . module CovidRt Compute time and area specific estimates of the reproductive number of Covid. source # CovidRt.lagdiff \u2014 Function . function lagdiff(x::AbstractVector, d=1) Returns out[i] = x[i] - x[i-d] with out padded with missings so that length(out)==length(x) . source # CovidRt.smooth \u2014 Method . function smooth(x::AbstractVector; w=pdf(Normal(), range(-3, 3, length=7))) Returns the running weighted mean of x*w. When n=length(w) , then out[i] = sum(x[i - n\u00f72 + j-1] * w[j] for j in 1:n)/sum(w) . source","title":"Package Docs"},{"location":"#covidrtjl","text":"CovidRt.CovidRt CovidRt.lagdiff CovidRt.smooth # CovidRt.CovidRt \u2014 Module . module CovidRt Compute time and area specific estimates of the reproductive number of Covid. source # CovidRt.lagdiff \u2014 Function . function lagdiff(x::AbstractVector, d=1) Returns out[i] = x[i] - x[i-d] with out padded with missings so that length(out)==length(x) . source # CovidRt.smooth \u2014 Method . function smooth(x::AbstractVector; w=pdf(Normal(), range(-3, 3, length=7))) Returns the running weighted mean of x*w. When n=length(w) , then out[i] = sum(x[i - n\u00f72 + j-1] * w[j] for j in 1:n)/sum(w) . source","title":"CovidRt.jl"},{"location":"Rt/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License Model \u00b6 Following Kevin Systrom , we adapt the approach of (Bettencourt 2008 ) to compute real-time rolling estimates of pandemic parameters. (Bettencourt 2008 ) begin from a SIR model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{R} & = \\gamma I \\end{align*} To this we add the possibility that not all cases are known. Cases get get detected at rate $I$, so cumulative confirmed cases, $C$, evolves as \\dot{C} = \\tau I Question Should we add other states to this model? If yes, how? I think using death and hospitalization numbers in estimation makes sense. The number of new confirmed cases from time $t$ to $t+\\delta$ is then: We will allow for the testing rate, $\\tau$, and infection rate, $\\beta$, to vary over time. k_t \\equiv \\frac{C(t+\\delta) - C(t)}{\\delta} = \\int_t^{t+\\delta} \\tau(s) I(s) ds \\approx \\tau(t) I(t) As in (Bettencourt 2008 ), \\begin{align*} I(t) = & I(t-\\delta) \\int_{t-\\delta}^{t} e^{\\frac{S(s)}{N} \\beta(s) - \\gamma} ds \\\\ \\approx & I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Note The reproductive number is: $R_t \\equiv \\frac{S(t)}{N}\\frac{\\beta(t)}{\\gamma}$. Substituting the expression for $I_t$ into $k_t$, we have \\begin{align*} k_t \\approx & \\tau(t) I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma\\right)} \\\\ \\approx & k_{t-\\delta} \\frac{\\tau(t)}{\\tau(t-\\delta)} e^{\\delta \\left(\\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Data \u00b6 We use data as on US states on Covid cases, deaths, policies, and other variables. The data combines information on Daily case counts and deaths from NYTimes Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase Statistical Model \u00b6 The above theoretical model gives a deterministic relationship between $k_t$ and $k_{t-1}$ given the parameters. To bring it to data we must add stochasticity. Systrom\u2019s approach \u00b6 First we describe what Systrom does. He assumes that $R_{0} \\sim Gamma(4,1)$. Then for $t=1, \u2026, T$, he computes $P(R_t|k_{t}, k_{t-1}, \u2026 ,k_0)$ iteratively using Bayes\u2019 rules. Specifically, he assumes k_t | R_t, k_{t-1}, ... \\sim Poisson(k_{t-1} e^{\\gamma(R_t - 1)}) and that $R_t$ follows a random walk, so the prior of $R_t | R_{t-1}$ is R_t | R_{t-1} \\sim N(R_{t-1}, \\sigma^2) so that P(R_t|k_{t}, k_{t-1}, ... ,k_0) = \\frac{P(k_t | R_t, k_{t-1}) P(R_t | R_{t-1}) P(R_{t-1} | k_{t-1}, ...)} {P(k_t)} Note that this computes posteriors of $R_t$ given current and past cases. Future cases are also informative of $R_t$, and you could instead compute $P(R_t | k_0, k_1, \u2026, k_T)$. The notebook makes some mentions of Gaussian processes. There\u2019s likely some way to recast the random walk assumption as a Gaussian process prior (the kernel would be $\\kappa(t,t\u2019) = \\min{t,t\u2019} \\sigma^2$), but that seems to me like an unusual way to describe it. Code \u00b6 Let\u2019s see how Systrom\u2019s method works. First the load data. using Pkg try using CovidData catch pkg\"registry add https://github.com/schrimpf/julia-registry.git\" Pkg.develop(\"CovidData\") Pkg.develop(\"CovidRt\") end using DataFrames, Plots, StatsPlots, CovidRt, CovidData Plots.pyplot() df = CovidData.statedata() df = filter(x->x.fips<60, df) # focus on 10 states with most cases as of April 1, 2020 sdf = select(df[df[!,:date].==Dates.Date(\"2020-04-01\"),:], Symbol(\"cases.nyt\"), :state) |> x->sort(x,Symbol(\"cases.nyt\"), rev=true) states=sdf[1:10,:state] sdf = select(filter(r->r[:state] \u2208 states, df), Symbol(\"cases.nyt\"), :state, :date) sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] figs = [] for gdf in groupby(sdf, :state) f = @df gdf plot(:date, :newcases, legend=:none, linewidth=2, title=unique(gdf.state)[1]) global figs = vcat(figs,f) end display(plot(figs[1:9]..., layout=(3,3))) From this we can see that new cases are very noisy. This is especially problematic when cases jump from near 0 to very high values, such as in Illinois. The median value of and variance of new cases, $k_t$, are both $k_{t-1} e^{\\gamma(R_t - 1)}$. Only huge changes in $R_t$ can rationalize huge jumps in new cases. Let\u2019s compute posteriors for each state. using Interpolations, Distributions function rtpost(cases, \u03b3, \u03c3, prior0, casepdf) (rgrid, postgrid, ll) = rtpostgrid(cases)(\u03b3, \u03c3, prior0, casepdf) w = rgrid[2] - rgrid[1] T = length(cases) p = [LinearInterpolation(rgrid, postgrid[:,t]) for t in 1:T] coverage = 0.9 cr = zeros(T,2) mu = vec(rgrid' * postgrid*w) for t in 1:T l = findfirst(cumsum(postgrid[:,t].*w).>(1-coverage)/2) h = findlast(cumsum(postgrid[:,t].*w).<(1-(1-coverage)/2)) if !(l === nothing || h === nothing) cr[t,:] = [rgrid[l], rgrid[h]] end end return(p, mu, cr) end function rtpostgrid(cases) # We'll compute the posterior on these values of R_t rlo = 0 rhi = 8 steps = 500 rgrid = range(rlo, rhi, length=steps) \u0394grid = range(0.05, 0.95, length=10) w = rgrid[2] - rgrid[1] dr = rgrid .- rgrid' fn=function(\u03b3, \u03c3, prior0, casepdf) prr = pdf.(Normal(0,\u03c3), dr) # P(r_{t+1} | r_t) for i in 1:size(prr,1) prr[i, : ] ./= sum(prr[i,:].*w) end postgrid = Matrix{typeof(\u03c3)}(undef,length(rgrid), length(cases)) # P(R_t | k_t, k_{t-1},...) like = similar(postgrid, length(cases)) for t in 1:length(cases) if (t==1) postgrid[:,t] .= prior0.(rgrid) else if (cases[t-1]===missing || cases[t]===missing) pkr = 1 # P(k_t | R_t) else \u03bb = max(cases[t-1],1).* exp.(\u03b3 .* (rgrid .- 1)) #r = \u03bb*nbp/(1-nbp) #pkr = pdf.(NegativeBinomial.(r,nbp), cases[t]) pkr = casepdf.(\u03bb, cases[t]) if (all(pkr.==0)) @warn \"all pkr=0\" #@show t, cases[t], cases[t-1] pkr .= 1 end end postgrid[:,t] = pkr.*(prr*postgrid[:,t-1]) like[t] = sum(postgrid[:,t].*w) postgrid[:,t] ./= max(like[t], 1e-15) end end ll = try sum(log.(like)) catch -710*length(like) end return((rgrid, postgrid, ll)) end return(fn) end for \u03c3 in [0.1, 0.25, 1] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l)) end In these results, what is happening is that when new cases fluctuate too much, the likelihood is identically 0, causing the posterior calculation to break down. Increasing the variance of changes in $R_t$, widens the posterior confidence intervals, but does not solve the problem of vanishing likelihoods. One thing that can \u201csolve\u201d the problem is choosing a distribution of $k_t | \\lambda, k_{t-1}$ with higher variance. The negative binomial with parameters $\\lambda p/(1-p)$ and $p$ has mean $\\lambda$ and variance $\\lambda/p$. \u03b3 =1/7 \u03c3 = 0.25 Plots.closeall() for \u03c3 in [0.1, 0.25, 0.5] for nbp in [0.5, 0.1, 0.01] figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(NegativeBinomial(\u03bb*nbp/(1-nbp), nbp),x)); f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Negative binomial, p=$nbp, & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end What Systrom did was smooth the new cases before using the Poisson distribution. He used a window width of $7$ and Gaussian weights with standard deviation $2$. \u03c3 = 0.25 Plots.closeall() for w in [3, 7, 11] for s in [0.5, 2, 4] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) windowsize = w weights = pdf(Normal(0, s), -floor(windowsize/2):floor(windowsize/2)) weights = weights/sum(weights) smoothcases = smooth(gdf.newcases, w=weights) p, m, cr = rtpost(smoothcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3, s=$s, w=$w\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end Here we see that we can get a variety of results depending on the smoothing used. All of these posteriors ignore the uncertainty in the choice of smoothing parameters (and procedure). An alternative approach \u00b6 Here we follow an approach similar in spirit to Systrom, with a few modifications and additions. The primary modification is that we alter the model of $k_t|k_{t-1}, R_t$ to allow measurement error in both $k_t$ and $k_{t-1}$. We make four additions. First, we utilize data on movement and business operations as auxillary noisy measures of $R_t$. Second, we allow state policies to shift the mean of $R_t$. Third, we combine data from all states to improve precision in each. Fourth, we incorporate testing numbers into the data. As above, we begin from the approximation k^*_{s,t} \\approx k^*_{s,t-1} \\frac{\\tau_{s,t}}{\\tau_{s,t-1}} e^{\\gamma(R_{st} - 1)}) where $k^*$ is the true, unobserved number of new cases. Taking logs and rearranging we have \\log(k^*_{s,t}) - \\log(k^*_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) Let $k_{s,t}$ be the noisy observed value of $k^*_{s,t}$, then \\log(k_{s,t}) - \\log(k_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) - \\epsilon_{s,t} + \\epsilon_{s,t-1} where \\log(k^*_{s,t}) = \\log(k_{s,t}) +\\epsilon_{s,t} and $\\epsilon_{s,t}$ is measurement error. With appropriate assumptions on $\\epsilon$, $\\tau$, $R$ and other observables, we can then use regression to estimate $R$. As a simple example, let\u2019s assume $R_{s,t} = R_{s,0} + \\alpha d_{s,t}$ where $d_{s,t}$ are indicators for NPI\u2019s being in place. That $\\tau_{s,t}$ is constant over time for each $s$ $E[\\epsilon_{s,t} - \\epsilon_{s,t-1}|d] = 0$ and $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is uncorrelated over time (just to simplify; this is not a good assumption). ```{=html} ``` julia using GLM, RegressionTables pvars = [Symbol(\"Stay.at.home..shelter.in.place\"), Symbol(\"State.of.emergency\"), Symbol(\"Date.closed.K.12.schools\"), Symbol(\"Closed.gyms\"), Symbol(\"Closed.movie.theaters\"), Symbol(\"Closed.day.cares\"), Symbol(\"Date.banned.visitors.to.nursing.homes\"), Symbol(\"Closed.non.essential.businesses\"), Symbol(\"Closed.restaurants.except.take.out\")] sdf = copy(df) for p in pvars sdf[!,p] = by(sdf, :state, (:date, p) => x->(!ismissing(unique(x[p])[1]) .& (x.date .>= unique(x[p])[1]))).x1 end sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] sdf[!,:dlogk] = by(sdf, :state, dlogk = :newcases => x->(vcat(missing, diff(log.(max.(x,0.1))))))[!,:dlogk] fmla = FormulaTerm(Term(:dlogk), Tuple(Term.(vcat(pvars,:state)))) reg = lm(fmla, sdf) regtable(reg, renderSettings=asciiOutput()) ----------------------------------------------- dlogk ------- (1) ----------------------------------------------- (Intercept) 0.175 (0.171) Stay.at.home..shelter.in.place 0.009 (0.060) State.of.emergency -0.093 (0.077) Date.closed.K.12.schools 0.047 (0.081) Closed.gyms -0.080 (0.120) Closed.movie.theaters 0.111 (0.127) Closed.day.cares 0.100 (0.056) Date.banned.visitors.to.nursing.homes -0.009 (0.047) Closed.non.essential.businesses -0.060 (0.069) Closed.restaurants.except.take.out -0.072 (0.107) state: Alaska 0.012 (0.227) state: Arizona -0.023 (0.198) state: Arkansas -0.037 (0.226) state: California -0.025 (0.197) state: Colorado -0.002 (0.219) state: Connecticut 0.032 (0.222) state: Delaware 0.024 (0.225) state: District of Columbia 0.052 (0.221) state: Florida 0.067 (0.216) state: Georgia 0.059 (0.217) state: Hawaii -0.025 (0.220) state: Idaho -0.036 (0.228) state: Illinois 0.007 (0.197) state: Indiana 0.089 (0.220) state: Iowa 0.014 (0.222) state: Kansas 0.073 (0.221) state: Kentucky 0.071 (0.220) state: Louisiana -0.001 (0.223) state: Maine -0.019 (0.227) state: Maryland 0.083 (0.219) state: Massachusetts 0.020 (0.200) state: Michigan 0.112 (0.224) state: Minnesota 0.073 (0.220) state: Mississippi 0.078 (0.226) state: Missouri 0.063 (0.221) state: Montana -0.089 (0.228) state: Nebraska 0.002 (0.207) state: Nevada 0.041 (0.219) state: New Hampshire -0.020 (0.217) state: New Jersey 0.048 (0.218) state: New Mexico 0.006 (0.226) state: New York 0.088 (0.216) state: North Carolina 0.057 (0.218) state: North Dakota 0.047 (0.226) state: Ohio 0.080 (0.224) state: Oklahoma 0.040 (0.221) state: Oregon -0.004 (0.215) state: Pennsylvania 0.028 (0.221) state: Rhode Island 0.036 (0.216) state: South Carolina 0.036 (0.221) state: South Dakota -0.035 (0.225) state: Tennessee 0.050 (0.220) state: Texas -0.018 (0.205) state: Utah 0.011 (0.212) state: Vermont 0.002 (0.222) state: Virginia 0.051 (0.222) state: Washington -0.022 (0.196) state: West Virginia -0.009 (0.234) state: Wisconsin -0.004 (0.201) state: Wyoming 0.015 (0.226) ----------------------------------------------- Estimator OLS ----------------------------------------------- N 2,876 R2 0.004 ----------------------------------------------- From this we get that if we assume $\\gamma = 1/7$, then the the baseline estimate of $R$ in Illinois is $7(0.046 + 0.034) + 1\\approx 1.56$ with a stay at home order, $R$ in Illinois becomes $7(0.046 + 0.035 - 0.147) + 1 \\approx 0.53$. Some of the policies have positive coefficient estimates, which is strange. This is likely due to assumption 1 being incorrect. There is likely an unobserved component of $R_{s,t}$ that is positively correlated with policy indicators. State space model \u00b6 A direct analog of Systrom\u2019s approach is to treat $R_{s,t}$ as an unobserved latent process. Specifically, we will assume that \\begin{align*} \\tilde{R}_{s,0} & \\sim N(\\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} Note that the Poisson assumption on the distribution of $k_{s,t}$ used by Systrom implies an extremely small $\\sigma^2_k$, since the variance of log Poisson($\\lambda$) distribution is $1/\\lambda$. If $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ weere independent over $t$, we could compute the likelihood and posteriors of $R_{s,t}$ through the standard Kalman filter. Of course, $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is not independent over time, so we must adjust the Kalman filter accordingly. We follow the approach of (Kurtz and Lin 2019 ) to make this adjustment. Question Is there a better reference? I\u2019m sure someone did this much earlier than 2019\u2026 We estimate the parameters using data from US states. We set time 0 as the first day in which a state had at least 10 cumulative cases. We then compute posteriors for the parameters by MCMC. We place the following priors on the parameters. using Distributions, TransformVariables, DynamicHMC, MCMCChains, Plots, StatsPlots, LogDensityProblems, Random, LinearAlgebra, JLD2 rlo=-1 rhi=1.1 priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal([1], 3), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03b1 = MvNormal([1], 3), \u03c1 = Uniform(rlo, rhi)) (\u03b3 = Truncated(Distributions.Normal{Float64}(\u03bc=0.14285714285714285, \u03c3=0.142 85714285714285), range=(0.03571428571428571, 1.0)), \u03c3R0 = Truncated(Distrib utions.Normal{Float64}(\u03bc=1.0, \u03c3=3.0), range=(0.0, Inf)), \u03b10 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c3R = Truncated(Distributions.Normal{Float64}(\u03bc=0.25, \u03c3=1.0), range=(0.0, Inf)), \u03c3k = Truncated(Distributions.Normal{Float64}(\u03bc=0.1, \u03c3=5.0), range=(0 .0, Inf)), \u03b1 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c1 = Distributions.Uniform{Float64}(a=-1.0, b=1.1)) The estimation is fast and the chain appears to mix well. reestimate=false sdf = sort(sdf, (:state, :date)); dlogk = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).dlogk for st in unique(sdf.state)]; dates = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).date for st in unique(sdf.state)]; mdl = RtModel(dlogk, priors) trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, 1), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03b1 = as(Array,1), \u03c1=as(Real, rlo, rhi)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10=[4.0],\u03c3R=0.25, \u03c3k=2.0, \u03b1=[1], \u03c1=0.9) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 3.330265 seconds (6.58 M allocations: 327.353 MiB, 4.54% gc time) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=3, M=Symmetric) x0 = x0 if (!isfile(\"rt1.jld2\") || reestimate) res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt1.jld2\" post end @load \"rt1.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(cc) Object of type Chains, with data of type 2000\u00d77\u00d71 reshape(::LinearAlgebra.A djoint{Float64,Array{Float64,2}}, 2000, 7, 1) with eltype Float64 Iterations = 1:2000 Thinning interval = 1 Chains = 1 Samples per chain = 2000 parameters = \u03b3, \u03c3R0, \u03b10, \u03c3R, \u03c3k, \u03b1, \u03c1 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0919 0.0339 0.0008 0.0017 455.3275 1.0009 \u03c3R0 1.0223 0.6973 0.0156 0.0317 673.2799 0.9996 \u03b10 5.1918 1.7792 0.0398 0.0909 401.6285 1.0001 \u03c3R 0.0887 0.0735 0.0016 0.0027 821.3143 1.0004 \u03c3k 0.5507 0.0082 0.0002 0.0001 3009.0575 0.9997 \u03b1 0.4512 0.4386 0.0098 0.0249 307.9202 0.9996 \u03c1 0.9394 0.0122 0.0003 0.0005 787.5179 1.0002 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0451 0.0665 0.0849 0.1101 0.1721 \u03c3R0 0.0534 0.5141 0.8979 1.3925 2.7113 \u03b10 2.4360 3.8546 4.9722 6.3808 9.1091 \u03c3R 0.0028 0.0327 0.0722 0.1248 0.2819 \u03c3k 0.5345 0.5453 0.5505 0.5560 0.5677 \u03b1 -0.5931 0.2494 0.5305 0.7539 1.0493 \u03c1 0.9140 0.9315 0.9402 0.9481 0.9618 display(plot(cc)) The posterior for the initial distribution of $R_{0,s}$ is not very precise. The other parameters have fairly precise posteriors. Systrom fixed all these parameters, except $\\sigma_R$, which he estimated by maximum likelihood to be 0.25. In these posteriors, a 95% credible region for $\\sigma_R$ contains his estimate. The posterior of $\\rho$ is not far from his imposed value of $1$, although $1$ is out of the 95% credible region. A 95% posterior region for $\\gamma$ contains Systrom\u2019s calibrated value of $1/7$. It is worth noting that the estimate of $\\sigma_k$ is large compared to $\\sigma_r$. This will cause new observations of $\\Delta \\log k$ will have a small effect on the posterior mean of $R$. Given values of the parameters, we can compute state and time specific posterior estimates of $R_{s,t}$. states = unique(sdf.state) s = findfirst(states.==\"New York\") figr = plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1), [1]) display(plot(figr, ylim=(-1,10))) This figure shows the posterior distribution of $R_{s,t}$ in New York. The black line is the posterior mean. The dark grey region is the average (over model parameters) of a 90% credible region conditional on the model parameters. This is comparable to what Systrom (and many others) report, and ignores uncertainty in the model parameters. The light grey region is a 90% credile region taking into account parameter uncertainty. The points and error bars are mean and 90% credible regions for \\Delta \\log k_{t}/\\gamma + 1 = R_{t} + \\epsilon_t/\\gamma Posteriors for additional states \u00b6 states_to_plot = [\"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), 9*(S \u00f7 9 + 1)) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1),[1]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) if ((i % 9) ==0 || ( i==length(states_to_plot))) display(plot(figs[(i-8):i]..., layout=(3,3), reuse=false)) end end We can see that the posteriors vary very little from state to state. The model picks up a general downward trend in $\\Delta \\log k$ through the slightly less than 1 estimate of $\\rho$. This drives the posteriors of $R_{s,t}$ in every state to decrease over time. Since $\\sigma_k >> \\sigma_R$, the actual realizations of $\\Delta \\log k$ do not affect the state-specific posteriors very much. Note I also tried fixing $\\rho=1$. This gives similar results in terms of $\\sigma_k >> \\sigma_R$, and gives a posterior for $R_{s,t}$ that is approximately constant over time. Bettencourt, Ruy M., Lu\u00eds M. A. AND Ribeiro. 2008. \u201cReal Time Bayesian Estimation of the Epidemic Potential of Emerging Infectious Diseases.\u201d PLOS ONE 3 (5): 1\u20139. https://doi.org/10.1371/journal.pone.0002185 . Kurtz, Vince, and Hai Lin. 2019. \u201cKalman Filtering with Gaussian Processes Measurement Noise.\u201d","title":"Systrom Approach"},{"location":"Rt/#model","text":"Following Kevin Systrom , we adapt the approach of (Bettencourt 2008 ) to compute real-time rolling estimates of pandemic parameters. (Bettencourt 2008 ) begin from a SIR model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{R} & = \\gamma I \\end{align*} To this we add the possibility that not all cases are known. Cases get get detected at rate $I$, so cumulative confirmed cases, $C$, evolves as \\dot{C} = \\tau I Question Should we add other states to this model? If yes, how? I think using death and hospitalization numbers in estimation makes sense. The number of new confirmed cases from time $t$ to $t+\\delta$ is then: We will allow for the testing rate, $\\tau$, and infection rate, $\\beta$, to vary over time. k_t \\equiv \\frac{C(t+\\delta) - C(t)}{\\delta} = \\int_t^{t+\\delta} \\tau(s) I(s) ds \\approx \\tau(t) I(t) As in (Bettencourt 2008 ), \\begin{align*} I(t) = & I(t-\\delta) \\int_{t-\\delta}^{t} e^{\\frac{S(s)}{N} \\beta(s) - \\gamma} ds \\\\ \\approx & I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Note The reproductive number is: $R_t \\equiv \\frac{S(t)}{N}\\frac{\\beta(t)}{\\gamma}$. Substituting the expression for $I_t$ into $k_t$, we have \\begin{align*} k_t \\approx & \\tau(t) I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma\\right)} \\\\ \\approx & k_{t-\\delta} \\frac{\\tau(t)}{\\tau(t-\\delta)} e^{\\delta \\left(\\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*}","title":"Model"},{"location":"Rt/#data","text":"We use data as on US states on Covid cases, deaths, policies, and other variables. The data combines information on Daily case counts and deaths from NYTimes Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase","title":"Data"},{"location":"Rt/#statistical-model","text":"The above theoretical model gives a deterministic relationship between $k_t$ and $k_{t-1}$ given the parameters. To bring it to data we must add stochasticity.","title":"Statistical Model"},{"location":"Rt/#systroms-approach","text":"First we describe what Systrom does. He assumes that $R_{0} \\sim Gamma(4,1)$. Then for $t=1, \u2026, T$, he computes $P(R_t|k_{t}, k_{t-1}, \u2026 ,k_0)$ iteratively using Bayes\u2019 rules. Specifically, he assumes k_t | R_t, k_{t-1}, ... \\sim Poisson(k_{t-1} e^{\\gamma(R_t - 1)}) and that $R_t$ follows a random walk, so the prior of $R_t | R_{t-1}$ is R_t | R_{t-1} \\sim N(R_{t-1}, \\sigma^2) so that P(R_t|k_{t}, k_{t-1}, ... ,k_0) = \\frac{P(k_t | R_t, k_{t-1}) P(R_t | R_{t-1}) P(R_{t-1} | k_{t-1}, ...)} {P(k_t)} Note that this computes posteriors of $R_t$ given current and past cases. Future cases are also informative of $R_t$, and you could instead compute $P(R_t | k_0, k_1, \u2026, k_T)$. The notebook makes some mentions of Gaussian processes. There\u2019s likely some way to recast the random walk assumption as a Gaussian process prior (the kernel would be $\\kappa(t,t\u2019) = \\min{t,t\u2019} \\sigma^2$), but that seems to me like an unusual way to describe it.","title":"Systrom\u2019s approach"},{"location":"Rt/#code","text":"Let\u2019s see how Systrom\u2019s method works. First the load data. using Pkg try using CovidData catch pkg\"registry add https://github.com/schrimpf/julia-registry.git\" Pkg.develop(\"CovidData\") Pkg.develop(\"CovidRt\") end using DataFrames, Plots, StatsPlots, CovidRt, CovidData Plots.pyplot() df = CovidData.statedata() df = filter(x->x.fips<60, df) # focus on 10 states with most cases as of April 1, 2020 sdf = select(df[df[!,:date].==Dates.Date(\"2020-04-01\"),:], Symbol(\"cases.nyt\"), :state) |> x->sort(x,Symbol(\"cases.nyt\"), rev=true) states=sdf[1:10,:state] sdf = select(filter(r->r[:state] \u2208 states, df), Symbol(\"cases.nyt\"), :state, :date) sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] figs = [] for gdf in groupby(sdf, :state) f = @df gdf plot(:date, :newcases, legend=:none, linewidth=2, title=unique(gdf.state)[1]) global figs = vcat(figs,f) end display(plot(figs[1:9]..., layout=(3,3))) From this we can see that new cases are very noisy. This is especially problematic when cases jump from near 0 to very high values, such as in Illinois. The median value of and variance of new cases, $k_t$, are both $k_{t-1} e^{\\gamma(R_t - 1)}$. Only huge changes in $R_t$ can rationalize huge jumps in new cases. Let\u2019s compute posteriors for each state. using Interpolations, Distributions function rtpost(cases, \u03b3, \u03c3, prior0, casepdf) (rgrid, postgrid, ll) = rtpostgrid(cases)(\u03b3, \u03c3, prior0, casepdf) w = rgrid[2] - rgrid[1] T = length(cases) p = [LinearInterpolation(rgrid, postgrid[:,t]) for t in 1:T] coverage = 0.9 cr = zeros(T,2) mu = vec(rgrid' * postgrid*w) for t in 1:T l = findfirst(cumsum(postgrid[:,t].*w).>(1-coverage)/2) h = findlast(cumsum(postgrid[:,t].*w).<(1-(1-coverage)/2)) if !(l === nothing || h === nothing) cr[t,:] = [rgrid[l], rgrid[h]] end end return(p, mu, cr) end function rtpostgrid(cases) # We'll compute the posterior on these values of R_t rlo = 0 rhi = 8 steps = 500 rgrid = range(rlo, rhi, length=steps) \u0394grid = range(0.05, 0.95, length=10) w = rgrid[2] - rgrid[1] dr = rgrid .- rgrid' fn=function(\u03b3, \u03c3, prior0, casepdf) prr = pdf.(Normal(0,\u03c3), dr) # P(r_{t+1} | r_t) for i in 1:size(prr,1) prr[i, : ] ./= sum(prr[i,:].*w) end postgrid = Matrix{typeof(\u03c3)}(undef,length(rgrid), length(cases)) # P(R_t | k_t, k_{t-1},...) like = similar(postgrid, length(cases)) for t in 1:length(cases) if (t==1) postgrid[:,t] .= prior0.(rgrid) else if (cases[t-1]===missing || cases[t]===missing) pkr = 1 # P(k_t | R_t) else \u03bb = max(cases[t-1],1).* exp.(\u03b3 .* (rgrid .- 1)) #r = \u03bb*nbp/(1-nbp) #pkr = pdf.(NegativeBinomial.(r,nbp), cases[t]) pkr = casepdf.(\u03bb, cases[t]) if (all(pkr.==0)) @warn \"all pkr=0\" #@show t, cases[t], cases[t-1] pkr .= 1 end end postgrid[:,t] = pkr.*(prr*postgrid[:,t-1]) like[t] = sum(postgrid[:,t].*w) postgrid[:,t] ./= max(like[t], 1e-15) end end ll = try sum(log.(like)) catch -710*length(like) end return((rgrid, postgrid, ll)) end return(fn) end for \u03c3 in [0.1, 0.25, 1] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l)) end In these results, what is happening is that when new cases fluctuate too much, the likelihood is identically 0, causing the posterior calculation to break down. Increasing the variance of changes in $R_t$, widens the posterior confidence intervals, but does not solve the problem of vanishing likelihoods. One thing that can \u201csolve\u201d the problem is choosing a distribution of $k_t | \\lambda, k_{t-1}$ with higher variance. The negative binomial with parameters $\\lambda p/(1-p)$ and $p$ has mean $\\lambda$ and variance $\\lambda/p$. \u03b3 =1/7 \u03c3 = 0.25 Plots.closeall() for \u03c3 in [0.1, 0.25, 0.5] for nbp in [0.5, 0.1, 0.01] figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(NegativeBinomial(\u03bb*nbp/(1-nbp), nbp),x)); f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Negative binomial, p=$nbp, & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end What Systrom did was smooth the new cases before using the Poisson distribution. He used a window width of $7$ and Gaussian weights with standard deviation $2$. \u03c3 = 0.25 Plots.closeall() for w in [3, 7, 11] for s in [0.5, 2, 4] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) windowsize = w weights = pdf(Normal(0, s), -floor(windowsize/2):floor(windowsize/2)) weights = weights/sum(weights) smoothcases = smooth(gdf.newcases, w=weights) p, m, cr = rtpost(smoothcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3, s=$s, w=$w\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end Here we see that we can get a variety of results depending on the smoothing used. All of these posteriors ignore the uncertainty in the choice of smoothing parameters (and procedure).","title":"Code"},{"location":"Rt/#an-alternative-approach","text":"Here we follow an approach similar in spirit to Systrom, with a few modifications and additions. The primary modification is that we alter the model of $k_t|k_{t-1}, R_t$ to allow measurement error in both $k_t$ and $k_{t-1}$. We make four additions. First, we utilize data on movement and business operations as auxillary noisy measures of $R_t$. Second, we allow state policies to shift the mean of $R_t$. Third, we combine data from all states to improve precision in each. Fourth, we incorporate testing numbers into the data. As above, we begin from the approximation k^*_{s,t} \\approx k^*_{s,t-1} \\frac{\\tau_{s,t}}{\\tau_{s,t-1}} e^{\\gamma(R_{st} - 1)}) where $k^*$ is the true, unobserved number of new cases. Taking logs and rearranging we have \\log(k^*_{s,t}) - \\log(k^*_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) Let $k_{s,t}$ be the noisy observed value of $k^*_{s,t}$, then \\log(k_{s,t}) - \\log(k_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) - \\epsilon_{s,t} + \\epsilon_{s,t-1} where \\log(k^*_{s,t}) = \\log(k_{s,t}) +\\epsilon_{s,t} and $\\epsilon_{s,t}$ is measurement error. With appropriate assumptions on $\\epsilon$, $\\tau$, $R$ and other observables, we can then use regression to estimate $R$. As a simple example, let\u2019s assume $R_{s,t} = R_{s,0} + \\alpha d_{s,t}$ where $d_{s,t}$ are indicators for NPI\u2019s being in place. That $\\tau_{s,t}$ is constant over time for each $s$ $E[\\epsilon_{s,t} - \\epsilon_{s,t-1}|d] = 0$ and $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is uncorrelated over time (just to simplify; this is not a good assumption). ```{=html} ``` julia using GLM, RegressionTables pvars = [Symbol(\"Stay.at.home..shelter.in.place\"), Symbol(\"State.of.emergency\"), Symbol(\"Date.closed.K.12.schools\"), Symbol(\"Closed.gyms\"), Symbol(\"Closed.movie.theaters\"), Symbol(\"Closed.day.cares\"), Symbol(\"Date.banned.visitors.to.nursing.homes\"), Symbol(\"Closed.non.essential.businesses\"), Symbol(\"Closed.restaurants.except.take.out\")] sdf = copy(df) for p in pvars sdf[!,p] = by(sdf, :state, (:date, p) => x->(!ismissing(unique(x[p])[1]) .& (x.date .>= unique(x[p])[1]))).x1 end sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] sdf[!,:dlogk] = by(sdf, :state, dlogk = :newcases => x->(vcat(missing, diff(log.(max.(x,0.1))))))[!,:dlogk] fmla = FormulaTerm(Term(:dlogk), Tuple(Term.(vcat(pvars,:state)))) reg = lm(fmla, sdf) regtable(reg, renderSettings=asciiOutput()) ----------------------------------------------- dlogk ------- (1) ----------------------------------------------- (Intercept) 0.175 (0.171) Stay.at.home..shelter.in.place 0.009 (0.060) State.of.emergency -0.093 (0.077) Date.closed.K.12.schools 0.047 (0.081) Closed.gyms -0.080 (0.120) Closed.movie.theaters 0.111 (0.127) Closed.day.cares 0.100 (0.056) Date.banned.visitors.to.nursing.homes -0.009 (0.047) Closed.non.essential.businesses -0.060 (0.069) Closed.restaurants.except.take.out -0.072 (0.107) state: Alaska 0.012 (0.227) state: Arizona -0.023 (0.198) state: Arkansas -0.037 (0.226) state: California -0.025 (0.197) state: Colorado -0.002 (0.219) state: Connecticut 0.032 (0.222) state: Delaware 0.024 (0.225) state: District of Columbia 0.052 (0.221) state: Florida 0.067 (0.216) state: Georgia 0.059 (0.217) state: Hawaii -0.025 (0.220) state: Idaho -0.036 (0.228) state: Illinois 0.007 (0.197) state: Indiana 0.089 (0.220) state: Iowa 0.014 (0.222) state: Kansas 0.073 (0.221) state: Kentucky 0.071 (0.220) state: Louisiana -0.001 (0.223) state: Maine -0.019 (0.227) state: Maryland 0.083 (0.219) state: Massachusetts 0.020 (0.200) state: Michigan 0.112 (0.224) state: Minnesota 0.073 (0.220) state: Mississippi 0.078 (0.226) state: Missouri 0.063 (0.221) state: Montana -0.089 (0.228) state: Nebraska 0.002 (0.207) state: Nevada 0.041 (0.219) state: New Hampshire -0.020 (0.217) state: New Jersey 0.048 (0.218) state: New Mexico 0.006 (0.226) state: New York 0.088 (0.216) state: North Carolina 0.057 (0.218) state: North Dakota 0.047 (0.226) state: Ohio 0.080 (0.224) state: Oklahoma 0.040 (0.221) state: Oregon -0.004 (0.215) state: Pennsylvania 0.028 (0.221) state: Rhode Island 0.036 (0.216) state: South Carolina 0.036 (0.221) state: South Dakota -0.035 (0.225) state: Tennessee 0.050 (0.220) state: Texas -0.018 (0.205) state: Utah 0.011 (0.212) state: Vermont 0.002 (0.222) state: Virginia 0.051 (0.222) state: Washington -0.022 (0.196) state: West Virginia -0.009 (0.234) state: Wisconsin -0.004 (0.201) state: Wyoming 0.015 (0.226) ----------------------------------------------- Estimator OLS ----------------------------------------------- N 2,876 R2 0.004 ----------------------------------------------- From this we get that if we assume $\\gamma = 1/7$, then the the baseline estimate of $R$ in Illinois is $7(0.046 + 0.034) + 1\\approx 1.56$ with a stay at home order, $R$ in Illinois becomes $7(0.046 + 0.035 - 0.147) + 1 \\approx 0.53$. Some of the policies have positive coefficient estimates, which is strange. This is likely due to assumption 1 being incorrect. There is likely an unobserved component of $R_{s,t}$ that is positively correlated with policy indicators.","title":"An alternative approach"},{"location":"Rt/#state-space-model","text":"A direct analog of Systrom\u2019s approach is to treat $R_{s,t}$ as an unobserved latent process. Specifically, we will assume that \\begin{align*} \\tilde{R}_{s,0} & \\sim N(\\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} Note that the Poisson assumption on the distribution of $k_{s,t}$ used by Systrom implies an extremely small $\\sigma^2_k$, since the variance of log Poisson($\\lambda$) distribution is $1/\\lambda$. If $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ weere independent over $t$, we could compute the likelihood and posteriors of $R_{s,t}$ through the standard Kalman filter. Of course, $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is not independent over time, so we must adjust the Kalman filter accordingly. We follow the approach of (Kurtz and Lin 2019 ) to make this adjustment. Question Is there a better reference? I\u2019m sure someone did this much earlier than 2019\u2026 We estimate the parameters using data from US states. We set time 0 as the first day in which a state had at least 10 cumulative cases. We then compute posteriors for the parameters by MCMC. We place the following priors on the parameters. using Distributions, TransformVariables, DynamicHMC, MCMCChains, Plots, StatsPlots, LogDensityProblems, Random, LinearAlgebra, JLD2 rlo=-1 rhi=1.1 priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal([1], 3), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03b1 = MvNormal([1], 3), \u03c1 = Uniform(rlo, rhi)) (\u03b3 = Truncated(Distributions.Normal{Float64}(\u03bc=0.14285714285714285, \u03c3=0.142 85714285714285), range=(0.03571428571428571, 1.0)), \u03c3R0 = Truncated(Distrib utions.Normal{Float64}(\u03bc=1.0, \u03c3=3.0), range=(0.0, Inf)), \u03b10 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c3R = Truncated(Distributions.Normal{Float64}(\u03bc=0.25, \u03c3=1.0), range=(0.0, Inf)), \u03c3k = Truncated(Distributions.Normal{Float64}(\u03bc=0.1, \u03c3=5.0), range=(0 .0, Inf)), \u03b1 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c1 = Distributions.Uniform{Float64}(a=-1.0, b=1.1)) The estimation is fast and the chain appears to mix well. reestimate=false sdf = sort(sdf, (:state, :date)); dlogk = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).dlogk for st in unique(sdf.state)]; dates = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).date for st in unique(sdf.state)]; mdl = RtModel(dlogk, priors) trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, 1), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03b1 = as(Array,1), \u03c1=as(Real, rlo, rhi)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10=[4.0],\u03c3R=0.25, \u03c3k=2.0, \u03b1=[1], \u03c1=0.9) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 3.330265 seconds (6.58 M allocations: 327.353 MiB, 4.54% gc time) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=3, M=Symmetric) x0 = x0 if (!isfile(\"rt1.jld2\") || reestimate) res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt1.jld2\" post end @load \"rt1.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(cc) Object of type Chains, with data of type 2000\u00d77\u00d71 reshape(::LinearAlgebra.A djoint{Float64,Array{Float64,2}}, 2000, 7, 1) with eltype Float64 Iterations = 1:2000 Thinning interval = 1 Chains = 1 Samples per chain = 2000 parameters = \u03b3, \u03c3R0, \u03b10, \u03c3R, \u03c3k, \u03b1, \u03c1 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0919 0.0339 0.0008 0.0017 455.3275 1.0009 \u03c3R0 1.0223 0.6973 0.0156 0.0317 673.2799 0.9996 \u03b10 5.1918 1.7792 0.0398 0.0909 401.6285 1.0001 \u03c3R 0.0887 0.0735 0.0016 0.0027 821.3143 1.0004 \u03c3k 0.5507 0.0082 0.0002 0.0001 3009.0575 0.9997 \u03b1 0.4512 0.4386 0.0098 0.0249 307.9202 0.9996 \u03c1 0.9394 0.0122 0.0003 0.0005 787.5179 1.0002 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0451 0.0665 0.0849 0.1101 0.1721 \u03c3R0 0.0534 0.5141 0.8979 1.3925 2.7113 \u03b10 2.4360 3.8546 4.9722 6.3808 9.1091 \u03c3R 0.0028 0.0327 0.0722 0.1248 0.2819 \u03c3k 0.5345 0.5453 0.5505 0.5560 0.5677 \u03b1 -0.5931 0.2494 0.5305 0.7539 1.0493 \u03c1 0.9140 0.9315 0.9402 0.9481 0.9618 display(plot(cc)) The posterior for the initial distribution of $R_{0,s}$ is not very precise. The other parameters have fairly precise posteriors. Systrom fixed all these parameters, except $\\sigma_R$, which he estimated by maximum likelihood to be 0.25. In these posteriors, a 95% credible region for $\\sigma_R$ contains his estimate. The posterior of $\\rho$ is not far from his imposed value of $1$, although $1$ is out of the 95% credible region. A 95% posterior region for $\\gamma$ contains Systrom\u2019s calibrated value of $1/7$. It is worth noting that the estimate of $\\sigma_k$ is large compared to $\\sigma_r$. This will cause new observations of $\\Delta \\log k$ will have a small effect on the posterior mean of $R$. Given values of the parameters, we can compute state and time specific posterior estimates of $R_{s,t}$. states = unique(sdf.state) s = findfirst(states.==\"New York\") figr = plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1), [1]) display(plot(figr, ylim=(-1,10))) This figure shows the posterior distribution of $R_{s,t}$ in New York. The black line is the posterior mean. The dark grey region is the average (over model parameters) of a 90% credible region conditional on the model parameters. This is comparable to what Systrom (and many others) report, and ignores uncertainty in the model parameters. The light grey region is a 90% credile region taking into account parameter uncertainty. The points and error bars are mean and 90% credible regions for \\Delta \\log k_{t}/\\gamma + 1 = R_{t} + \\epsilon_t/\\gamma","title":"State space model"},{"location":"Rt/#posteriors-for-additional-states","text":"states_to_plot = [\"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), 9*(S \u00f7 9 + 1)) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1),[1]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) if ((i % 9) ==0 || ( i==length(states_to_plot))) display(plot(figs[(i-8):i]..., layout=(3,3), reuse=false)) end end We can see that the posteriors vary very little from state to state. The model picks up a general downward trend in $\\Delta \\log k$ through the slightly less than 1 estimate of $\\rho$. This drives the posteriors of $R_{s,t}$ in every state to decrease over time. Since $\\sigma_k >> \\sigma_R$, the actual realizations of $\\Delta \\log k$ do not affect the state-specific posteriors very much. Note I also tried fixing $\\rho=1$. This gives similar results in terms of $\\sigma_k >> \\sigma_R$, and gives a posterior for $R_{s,t}$ that is approximately constant over time. Bettencourt, Ruy M., Lu\u00eds M. A. AND Ribeiro. 2008. \u201cReal Time Bayesian Estimation of the Epidemic Potential of Emerging Infectious Diseases.\u201d PLOS ONE 3 (5): 1\u20139. https://doi.org/10.1371/journal.pone.0002185 . Kurtz, Vince, and Hai Lin. 2019. \u201cKalman Filtering with Gaussian Processes Measurement Noise.\u201d","title":"Posteriors for additional states"},{"location":"extensions/","text":"Another Derivation \u00b6 An alternative (and easier) way to derive the same estimator will be described here. This approach will easily generalize to more complicated models, but let\u2019s begin with the simplest SIR model with testing. \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\end{align*} Then note that \\ddot{C} = \\dot{\\tau} I + \\tau \\dot{I} and \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{\\dot{I}}{I} \\\\ \\frac{d}{dt} \\log(\\dot{C}) = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma \\\\ = & \\frac{\\dot{\\tau}}{\\tau} + \\gamma(R_t - 1) \\end{align*} which is the equation we have been using for estimation. Incorporating death \u00b6 If we add deaths to the model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I - p I\\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\\\ \\dot{D} & = p I \\end{align*} then, \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma - p \\\\ \\frac{\\ddot{D}}{\\dot{D}} = & \\frac{S}{N}\\beta - \\gamma - p \\\\ \\end{align*} Other observable states could similarly be added to the model. Time delays \u00b6 A drawback of the above approach is that it implies changes in $R_t$ show up in the derivatives of case and death numbers instantly. This is definitely not true. Instead consider a model where infections last $\\ell$ days. After $\\ell$ days, each infected person dies with probability $\\pi$ and recovers otherwise. Then we have \\begin{align*} \\dot{S}(t) & = -\\frac{S(t)}{N} \\beta I \\\\ \\dot{I}(t) & = \\frac{S(t)}{N} \\beta(t) I(t) - \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{C}(t) & = \\tau(t) I(t) \\\\ \\dot{\\mathcal{R}}(t) & = (1-\\pi) \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{D}(t) & = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\end{align*} Rearranging gives \\frac{\\ddot{C}(t)}{\\dot{C}(t)} = \\frac{\\dot{\\tau}(t)}{\\tau(t)} + \\frac{S(t)}{N}\\beta(t) - \\frac{1}{\\pi} \\frac{\\dot{D}(t)}{\\dot{C}(t)} and \\dot{D}(t) = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) \\frac{\\dot{C}(t-\\ell)}{\\tau(t-\\ell)} Note These last two equations also hold in the model without time delay by setting $\\ell=0$ and $\\pi = \\frac{p}{p+\\gamma}$ Note Random durations can be accomodated by replacing the shift by $\\ell$ with a convolution.","title":"Extensions"},{"location":"extensions/#another-derivation","text":"An alternative (and easier) way to derive the same estimator will be described here. This approach will easily generalize to more complicated models, but let\u2019s begin with the simplest SIR model with testing. \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\end{align*} Then note that \\ddot{C} = \\dot{\\tau} I + \\tau \\dot{I} and \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{\\dot{I}}{I} \\\\ \\frac{d}{dt} \\log(\\dot{C}) = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma \\\\ = & \\frac{\\dot{\\tau}}{\\tau} + \\gamma(R_t - 1) \\end{align*} which is the equation we have been using for estimation.","title":"Another Derivation"},{"location":"extensions/#incorporating-death","text":"If we add deaths to the model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I - p I\\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\\\ \\dot{D} & = p I \\end{align*} then, \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma - p \\\\ \\frac{\\ddot{D}}{\\dot{D}} = & \\frac{S}{N}\\beta - \\gamma - p \\\\ \\end{align*} Other observable states could similarly be added to the model.","title":"Incorporating death"},{"location":"extensions/#time-delays","text":"A drawback of the above approach is that it implies changes in $R_t$ show up in the derivatives of case and death numbers instantly. This is definitely not true. Instead consider a model where infections last $\\ell$ days. After $\\ell$ days, each infected person dies with probability $\\pi$ and recovers otherwise. Then we have \\begin{align*} \\dot{S}(t) & = -\\frac{S(t)}{N} \\beta I \\\\ \\dot{I}(t) & = \\frac{S(t)}{N} \\beta(t) I(t) - \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{C}(t) & = \\tau(t) I(t) \\\\ \\dot{\\mathcal{R}}(t) & = (1-\\pi) \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{D}(t) & = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\end{align*} Rearranging gives \\frac{\\ddot{C}(t)}{\\dot{C}(t)} = \\frac{\\dot{\\tau}(t)}{\\tau(t)} + \\frac{S(t)}{N}\\beta(t) - \\frac{1}{\\pi} \\frac{\\dot{D}(t)}{\\dot{C}(t)} and \\dot{D}(t) = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) \\frac{\\dot{C}(t-\\ell)}{\\tau(t-\\ell)} Note These last two equations also hold in the model without time delay by setting $\\ell=0$ and $\\pi = \\frac{p}{p+\\gamma}$ Note Random durations can be accomodated by replacing the shift by $\\ell$ with a convolution.","title":"Time delays"},{"location":"license/","text":"The notes and results are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation. The license for the package source code is here.","title":"License"},{"location":"rt_longdiff/","text":"Longer Differences and Smoothing \u00b6 One way to reduce $\\sigma_k/\\sigma_R$ is to reduce noise in new cases by taking a longer difference or smoothing case counts in some other way. How does this affect the estimation and interpretation of $R_t$? As in the first section, we start with the approximate recursive relation C(t) - C(t-1) \\equiv \\Delta C(t) \\approx \\frac{\\tau(t)}{\\tau(t-1)} e^{\\gamma (R_t - 1)} \\Delta C(t-1) If we instead look at a longer difference, \\begin{align*} C(t) - C(t-L) = & \\sum_{i=0}^{L-1} \\Delta C_{t-i} \\\\ \\approx & \\sum_{i=0}^{L-1} \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} \\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\sum_{i=0}^{L-1}\\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\left( C(t-1) - C(t-1-L) \\right) \\end{align*} where $\\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}}$ is some intermediate value in between the minimum and maximum of the ${ \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} }_{i=0}^{L-1}$. If testing is constant over time, we can then obtain an interpretable $\\overline{R_{t,L}}$ by using $k_{t,L} =\\log(C(t)-C(t-L))$ and following the procedure above. If testing varies with time, it becomes hard to separate testing rate changes from $R_t$ after taking long differnces. Note The same analysis can be applied to other smoothing operations, i.e. using \\sum_{i=0}^L w_i \\Delta C_{t-i} in place of $C(t) - C(t-L)$. However, there\u2019s something strange about smoothing $C_t$, and then extracting a smoothed component of it using the Kalman filter. The inference afterwards is suspect; we would essentially be estimating a kernel regression of $C_t$ on time, and using the estimated regression as though it\u2019s known with certainty. When would long differences reduce variance? Well if $\\Delta C(t) = \\Delta C^\\ast(t) + \\epsilon_t$ with $\\epsilon_t$ indepenedent over time with mean $0$ and constant variance, then you would need $C^\\ast(t) - C^\\ast(t-L)$ to increase faster than linearly with $L$. This is true if $C^\\ast$ is growing exponentially. Alternatively, if $\\epsilon_t$ is not independent over time, but negatively correlated (as seems likely), then variance can decrease with $L$. For example, if $\\Delta C(t) = C^\\ast(t) - C^\\ast(t-\\delta)$ with $\\delta$ a random, independent increment with mean $1$, then variance will tend to decrease with $L$ regardless of $C^\\ast(t)$. Results \u00b6 20-element Array{Symbol,1}: Symbol(\"Stay.at.home..shelter.in.place\") Symbol(\"Date.closed.K.12.schools\") Symbol(\"Closed.gyms\") Symbol(\"Closed.movie.theaters\") Symbol(\"Closed.day.cares\") Symbol(\"Date.banned.visitors.to.nursing.homes\") Symbol(\"Closed.non.essential.businesses\") Symbol(\"Closed.restaurants.except.take.out\") :retail_and_recreation_percent_change_from_baseline :grocery_and_pharmacy_percent_change_from_baseline :parks_percent_change_from_baseline :transit_stations_percent_change_from_baseline :workplaces_percent_change_from_baseline :residential_percent_change_from_baseline :percentchangebusinesses :constant :logpopdens Symbol(\"Percent.Unemployed..2018.\") Symbol(\"Percent.living.under.the.federal.poverty.line..2018.\") Symbol(\"Percent.at.risk.for.serious.illness.due.to.COVID\") Here, we will allow the initial and time varying mean of $R_{s,t}$ to depend on covariates. \\begin{align*} \\tilde{R}_{s,0} & \\sim N(X_{0,s} \\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = X_{s,t} \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} We present estimates of $R_t$ with \\Delta C(t) = C(t) - C(t-L_1) and \\Delta log (\\Delta C(t)) = log (\\Delta C(t)) - log (\\Delta C(t - L_2)) for a variety of values of $L_1$ and $L_2$ reestimate=false rlo=-1 #1 - eps(Float64) rhi=1.2 #1+ eps(Float64) K = length(xvars) priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal(zeros(length(x0vars)), sqrt(10)), #truncated(Normal(1, 3), 0, Inf), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03c1 = Uniform(rlo, rhi), \u03b1 = MvNormal(zeros(K), sqrt(10)) ) states_to_plot = [\"New York\", \"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] warmup = default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=100, middle_steps=100, terminating_steps=2*100, doubling_stages=3, M=Symmetric) for L1 in [1, 3, 7] for L2 in [1, 3, 7] mdl = CovidRt.RtModel(sdf, Symbol(\"cases.nyt\"), xvars, x0vars, priors; L1=L1, L2=L2, time0=r->(r[Symbol(\"cases.nyt\")].>=5)) estfile = \"rt$(L1)_$(L2).jld2\" if !isfile(estfile) || reestimate post = CovidRt.mcmc(mdl; iterations=2000, warmup=warmup) @save estfile post end @load estfile post cc = CovidRt.MCMCChain(post, xvars, x0vars) println(\"## L\u2081 = $(L1), L\u2082 = $(L2)\") println() display(plot(cc)) println(latexify(DataFrame(describe(cc)[1]), env=:mdtable, latex=false, fmt=x->round(x, sigdigits=3))) println(latexify(DataFrame(describe(cc)[2]), env=:mdtable, latex=false, fmt=x->round(x, sigdigits=3))) states = mdl.id S = length(states_to_plot) figs = fill(plot(), S) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = CovidRt.plotpostr(mdl.t[s],mdl.dlogk[s],post, mdl.X[s], mdl.X0[s]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) display(figs[i]) end end end L\u2081 = 1, L\u2082 = 1 \u00b6 Error: UndefVarError: latexify not defined","title":"Long-differening"},{"location":"rt_longdiff/#longer-differences-and-smoothing","text":"One way to reduce $\\sigma_k/\\sigma_R$ is to reduce noise in new cases by taking a longer difference or smoothing case counts in some other way. How does this affect the estimation and interpretation of $R_t$? As in the first section, we start with the approximate recursive relation C(t) - C(t-1) \\equiv \\Delta C(t) \\approx \\frac{\\tau(t)}{\\tau(t-1)} e^{\\gamma (R_t - 1)} \\Delta C(t-1) If we instead look at a longer difference, \\begin{align*} C(t) - C(t-L) = & \\sum_{i=0}^{L-1} \\Delta C_{t-i} \\\\ \\approx & \\sum_{i=0}^{L-1} \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} \\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\sum_{i=0}^{L-1}\\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\left( C(t-1) - C(t-1-L) \\right) \\end{align*} where $\\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}}$ is some intermediate value in between the minimum and maximum of the ${ \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} }_{i=0}^{L-1}$. If testing is constant over time, we can then obtain an interpretable $\\overline{R_{t,L}}$ by using $k_{t,L} =\\log(C(t)-C(t-L))$ and following the procedure above. If testing varies with time, it becomes hard to separate testing rate changes from $R_t$ after taking long differnces. Note The same analysis can be applied to other smoothing operations, i.e. using \\sum_{i=0}^L w_i \\Delta C_{t-i} in place of $C(t) - C(t-L)$. However, there\u2019s something strange about smoothing $C_t$, and then extracting a smoothed component of it using the Kalman filter. The inference afterwards is suspect; we would essentially be estimating a kernel regression of $C_t$ on time, and using the estimated regression as though it\u2019s known with certainty. When would long differences reduce variance? Well if $\\Delta C(t) = \\Delta C^\\ast(t) + \\epsilon_t$ with $\\epsilon_t$ indepenedent over time with mean $0$ and constant variance, then you would need $C^\\ast(t) - C^\\ast(t-L)$ to increase faster than linearly with $L$. This is true if $C^\\ast$ is growing exponentially. Alternatively, if $\\epsilon_t$ is not independent over time, but negatively correlated (as seems likely), then variance can decrease with $L$. For example, if $\\Delta C(t) = C^\\ast(t) - C^\\ast(t-\\delta)$ with $\\delta$ a random, independent increment with mean $1$, then variance will tend to decrease with $L$ regardless of $C^\\ast(t)$.","title":"Longer Differences and Smoothing"},{"location":"rt_longdiff/#results","text":"20-element Array{Symbol,1}: Symbol(\"Stay.at.home..shelter.in.place\") Symbol(\"Date.closed.K.12.schools\") Symbol(\"Closed.gyms\") Symbol(\"Closed.movie.theaters\") Symbol(\"Closed.day.cares\") Symbol(\"Date.banned.visitors.to.nursing.homes\") Symbol(\"Closed.non.essential.businesses\") Symbol(\"Closed.restaurants.except.take.out\") :retail_and_recreation_percent_change_from_baseline :grocery_and_pharmacy_percent_change_from_baseline :parks_percent_change_from_baseline :transit_stations_percent_change_from_baseline :workplaces_percent_change_from_baseline :residential_percent_change_from_baseline :percentchangebusinesses :constant :logpopdens Symbol(\"Percent.Unemployed..2018.\") Symbol(\"Percent.living.under.the.federal.poverty.line..2018.\") Symbol(\"Percent.at.risk.for.serious.illness.due.to.COVID\") Here, we will allow the initial and time varying mean of $R_{s,t}$ to depend on covariates. \\begin{align*} \\tilde{R}_{s,0} & \\sim N(X_{0,s} \\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = X_{s,t} \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} We present estimates of $R_t$ with \\Delta C(t) = C(t) - C(t-L_1) and \\Delta log (\\Delta C(t)) = log (\\Delta C(t)) - log (\\Delta C(t - L_2)) for a variety of values of $L_1$ and $L_2$ reestimate=false rlo=-1 #1 - eps(Float64) rhi=1.2 #1+ eps(Float64) K = length(xvars) priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal(zeros(length(x0vars)), sqrt(10)), #truncated(Normal(1, 3), 0, Inf), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03c1 = Uniform(rlo, rhi), \u03b1 = MvNormal(zeros(K), sqrt(10)) ) states_to_plot = [\"New York\", \"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] warmup = default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=100, middle_steps=100, terminating_steps=2*100, doubling_stages=3, M=Symmetric) for L1 in [1, 3, 7] for L2 in [1, 3, 7] mdl = CovidRt.RtModel(sdf, Symbol(\"cases.nyt\"), xvars, x0vars, priors; L1=L1, L2=L2, time0=r->(r[Symbol(\"cases.nyt\")].>=5)) estfile = \"rt$(L1)_$(L2).jld2\" if !isfile(estfile) || reestimate post = CovidRt.mcmc(mdl; iterations=2000, warmup=warmup) @save estfile post end @load estfile post cc = CovidRt.MCMCChain(post, xvars, x0vars) println(\"## L\u2081 = $(L1), L\u2082 = $(L2)\") println() display(plot(cc)) println(latexify(DataFrame(describe(cc)[1]), env=:mdtable, latex=false, fmt=x->round(x, sigdigits=3))) println(latexify(DataFrame(describe(cc)[2]), env=:mdtable, latex=false, fmt=x->round(x, sigdigits=3))) states = mdl.id S = length(states_to_plot) figs = fill(plot(), S) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = CovidRt.plotpostr(mdl.t[s],mdl.dlogk[s],post, mdl.X[s], mdl.X0[s]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) display(figs[i]) end end end","title":"Results"},{"location":"rt_longdiff/#l1-1-l2-1","text":"Error: UndefVarError: latexify not defined","title":"L\u2081 = 1, L\u2082 = 1"}]}